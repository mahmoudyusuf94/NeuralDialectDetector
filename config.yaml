adam_epsilon: 1.0e-08
adapter_type: plain_adapter
batch_size: 32
bottleneck_dim: 384
checkpoint_on_improvement: true
checkpointing_freq: 1
checkpointing_on: false
checkpointing_path: checkpoints_marbert
class_index: -1
classif_dropout_rate: 0.3
current_adapter_to_train: 6
device: cuda
early_stopping: true
early_stopping_patience: 10
handle_imbalance_sampler: false
improvement_check_freq: 100
indexes_filtration_path: null
initial_learning_rate: 5.0e-05
is_MSA: false
is_province: false
lr-mini-epoch-size: 50
inv-sqrt-lr-temperature: 2
inv-sqrt-lr-min-factor: 0.01
inv-sqrt-lr-max-factor: 1
masking_percentage: 0
max_ex_per_class: null
max_sequence_length: 90
model_class: ArabicDialectBERT
model_name_path: UBC-NLP/MARBERT
neptune_experiment_name: MARBERT_DA_Country_100
neptuneaiAPI: eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiYzNlOTI4MDUtMWEwMC00N2U3LWEzYTAtZjAyMzA2YzE1YjI3In0=
no_total_adapters: 21
num_epochs: 5
one_class_filtration: null
path_to_data: NADI2021_DEV.1.0/Subtask_1.2+2.2_DA
run_title: MARBERT_DA_Country_100
save_final_model: false
seed_list:
- 1234
- 611
- 520
- 300
- 760
- 42
- 1111
- 1234
- 10
- 1
- 20
- 5
- 2
- 100
- 512
stage_2_training: true
use_adapt_after_fusion: true
use_adapters: false
use_neptune: false
use_regional_mapping: false
use_vert_att: true
vatt-bottleneck_dim: 384
vatt-final-adapter: false
vatt-positional-keys: 'sinosoid'
# vatt-positional-keys: 'random'
vatt-use-common-transform: true
warmup_steps: 250
